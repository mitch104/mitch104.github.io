<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>How to execute terminal commands with your voice on Linux - Mitchell Harle</title><meta name="gridsome:hash" content="2cbdc03282b13aa09057a95b3886c7e2d475e94d"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.13"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" name="keywords" content="Gridsome,Vue,Tailwind,Tailwind CSS,JavaScript,HTML,CSS,Vue.js,VueJS,Portfolio,Python,React Native,Software Engineer,Developer"><meta data-vue-tag="ssr" name="description" content="Peronsal website for Bristol software engineer Mitchell Harle"><meta data-vue-tag="ssr" name="author" content="Mitchell Harle"><meta data-vue-tag="ssr" data-key="description" name="description" content="Personal website for software engineer Mitchell Harle"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/assets/static/favicon.62d22cb.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/assets/static/favicon.1539b60.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/assets/static/favicon.dc0cdc5.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/assets/static/favicon.7b22250.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nunito+Sans:400,700"><link rel="preload" href="/assets/css/0.styles.da5c488f.css" as="style"><link rel="preload" href="/assets/js/app.e423efc5.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--post-vue.ab74464a.js" as="script"><link rel="prefetch" href="/assets/js/page--src--pages--404-vue.a1203547.js"><link rel="prefetch" href="/assets/js/page--src--pages--blog-vue.5d9bda27.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.6dc97b2a.js"><link rel="prefetch" href="/assets/js/page--src--templates--tag-vue.1b642b0e.js"><link rel="stylesheet" href="/assets/css/0.styles.da5c488f.css"><script data-vue-tag="ssr" src="https://www.googletagmanager.com/gtag/js?id=UA-159413433-1" async></script><script data-vue-tag="ssr" src="/google-analytics.js"></script><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="content-wrapper bg-background-primary font-sans text-copy-primary leading-normal flex flex-col min-h-screen"><header class="border-t-14 border-green-700"><nav class="container mx-auto flex flex-wrap justify-between items-center py-8"><div></div><div class="block lg:hidden"><button class="flex items-center px-3 py-2 border rounded border-gray-500 hover:text-gray-600 hover:border-gray-600"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="current-color h-3 w-3"><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" fill="gray"></path></svg></button></div><ul class="uppercase tracking-wide font-bold w-full block flex-grow lg:flex lg:flex-initial lg:w-auto items-center mt-8 lg:mt-0 hidden"><li class="mr-8 mb-6 lg:mb-0"><a href="#" class="text-copy-primary hover:text-gray-600"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"></path></svg></a></li><li class="mr-8 mb-6 lg:mb-0"><a href="/" class="text-copy-primary hover:text-gray-600">Home</a></li><li class="mr-8 mb-6 lg:mb-0"><a href="/#tools" class="text-copy-primary hover:text-gray-600">Tools</a></li><li class="mr-8 mb-6 lg:mb-0"><a href="/#projects" class="text-copy-primary hover:text-gray-600">Projects</a></li><li class="mr-8 mb-6 lg:mb-0"><a href="/#about" class="text-copy-primary hover:text-gray-600">About</a></li><li class="mr-8 mb-6 lg:mb-0"><a href="/blog" class="text-copy-primary hover:text-gray-600">Blog</a></li><li class="mr-8 mb-6 lg:mb-0"><div class="relative" data-v-94ece32a><div data-v-3ae9f24e data-v-94ece32a></div><div class="relative w-80" data-v-94ece32a><input type="text" placeholder="Search" value="" class="bg-background-form border border-gray-500 rounded-full px-4 pl-10 py-2 outline-none focus:border-green-500 w-60 md:w-80 lg:w-80" data-v-94ece32a><div class="absolute top-0 ml-3" style="top:10px" data-v-94ece32a><svg fill="currentColor" viewBox="0 0 24 24" width="24" height="24" class="text-gray-500 h-5 w-5" data-v-94ece32a><path d="M16.32 14.9l5.39 5.4a1 1 0 0 1-1.42 1.4l-5.38-5.38a8 8 0 1 1 1.41-1.41zM10 16a6 6 0 1 0 0-12 6 6 0 0 0 0 12z" class="heroicon-ui" data-v-94ece32a></path></svg></div><!----></div><!----></div></li></ul><div></div></nav></header><div class="flex-grow"><div class="container-inner mx-auto my-16"><h1 class="text-4xl font-bold leading-tight">How to execute terminal commands with your voice on Linux</h1><div class="text-xl text-gray-600 mb-4">May 28, 2020</div><div class="flex flex-wrap mb-4 text-sm"><a href="/tag/Bash/" class="bg-gray-300 rounded-full mb-4 px-4 py-2 mr-4 hover:bg-green-300 whitespace-no-wrap">
        Bash
      </a><a href="/tag/Linux/" class="bg-gray-300 rounded-full mb-4 px-4 py-2 mr-4 hover:bg-green-300 whitespace-no-wrap">
        Linux
      </a><a href="/tag/Ubuntu/" class="bg-gray-300 rounded-full mb-4 px-4 py-2 mr-4 hover:bg-green-300 whitespace-no-wrap">
        Ubuntu
      </a><a href="/tag/AI/" class="bg-gray-300 rounded-full mb-4 px-4 py-2 mr-4 hover:bg-green-300 whitespace-no-wrap">
        AI
      </a><a href="/tag/NLP/" class="bg-gray-300 rounded-full mb-4 px-4 py-2 mr-4 hover:bg-green-300 whitespace-no-wrap">
        NLP
      </a><a href="/tag/Open%20Source/" class="bg-gray-300 rounded-full mb-4 px-4 py-2 mr-4 hover:bg-green-300 whitespace-no-wrap">
        Open Source
      </a></div><div class="markdown-body mb-8"><p>The open source NLP tool <a href="http://voice2json.org/" target="_blank" rel="nofollow noopener noreferrer">voice2json</a> recently caught my eye on <a href="https://news.ycombinator.com/item?id=27235970" target="_blank" rel="nofollow noopener noreferrer">Hacker News</a>:</p>
<blockquote>
<p>voice2json is a collection of command-line tools for offline speech/intent recognition on Linux. It is free, open source (MIT), and supports 17 human languages.</p>
</blockquote>
<p>It got me thinking, how hard would it be use this tool to execute terminal commands with just your voice, it turns out... not very!</p>
<h2 id="installation"><a href="#installation" aria-hidden="true"><span class="icon icon-link"></span></a>Installation</h2>
<p>See <a href="http://voice2json.org/install.html" target="_blank" rel="nofollow noopener noreferrer">docs</a> for full details of voice2json installation instructions.</p>
<p>On my first attempt I used the <a href="http://voice2json.org/install.html#docker-image" target="_blank" rel="nofollow noopener noreferrer">Docker image</a> however I did encounter <a href="https://github.com/synesthesiam/voice2json/issues/21" target="_blank" rel="nofollow noopener noreferrer">this issue</a> with audio input being picked up by Docker, so for a smoother on-boarding process I recommend using the <a href="http://voice2json.org/install.html#debian-package" target="_blank" rel="nofollow noopener noreferrer">Debian package</a> which you can find in their <a href="https://github.com/synesthesiam/voice2json/releases" target="_blank" rel="nofollow noopener noreferrer">GitHub releases</a>.</p>
<p>You will also need to <a href="http://voice2json.org/install.html#download-profile" target="_blank" rel="nofollow noopener noreferrer">download a profile</a>, for English I went for their recommendation of <a href="https://github.com/synesthesiam/en-us_pocketsphinx-cmu" target="_blank" rel="nofollow noopener noreferrer">Pocketsphinx</a> which has served me well so far. Download the <a href="https://github.com/synesthesiam/en-us_pocketsphinx-cmu/releases" target="_blank" rel="nofollow noopener noreferrer">.tar.gz file</a> and extract it to <code>$HOME/.config/voice2json</code>. This is essentially your trained offline machine learning model.</p>
<h2 id="basic-usage"><a href="#basic-usage" aria-hidden="true"><span class="icon icon-link"></span></a>Basic usage</h2>
<p>First, you will need to run the command:</p>
<pre><code class="language-bash">voice2json train-profile
</code></pre>
<p>which should only take a second or two.</p>
<p>To check that everything's working run:</p>
<pre><code class="language-bash">voice2json transcribe-stream --open
</code></pre>
<p>and this should transcribe your speech and output various related information in json format to <code>stdout</code>. Now, I have a Northern English accent so the transcriptions for me certainly weren't perfect, however the power comes from this tool when combining speech recognition with intent recognition.</p>
<p>You can define a set of phrases which relate to a given 'intent', so "Turn on the living room lamp" might be mapped to the intent <code>ChangeLightState</code>. These intent mappings are defined in the file <code>$HOME/.config/voice2json/sentences.ini</code> and you can see an example <a href="https://github.com/synesthesiam/en-us_pocketsphinx-cmu/blob/master/sentences.ini" target="_blank" rel="nofollow noopener noreferrer">here</a>. For more details see <a href="http://voice2json.org/#how-it-works" target="_blank" rel="nofollow noopener noreferrer">how it works</a>.</p>
<p>So the following command:</p>
<pre><code class="language-bash">voice2json transcribe-stream | voice2json recognize-intent
</code></pre>
<p>will try to match your spoken words to the closest matching phrase and hence intent. I found that when using a hand full of phrases the tool was very good at finding the right intent, even with my accent!</p>
<h2 id="how-to-execute-commands"><a href="#how-to-execute-commands" aria-hidden="true"><span class="icon icon-link"></span></a>How to execute commands</h2>
<p>Now that we've got the tool recognizing our intent from a set of possibilities how can we now use this to execute terminal commands?</p>
<h3 id="fifo-named-pipe"><a href="#fifo-named-pipe" aria-hidden="true"><span class="icon icon-link"></span></a>Fifo named pipe</h3>
<p>First off, we need to make a fifo (first in first out) <a href="https://man7.org/linux/man-pages/man7/fifo.7.html" target="_blank" rel="nofollow noopener noreferrer">named pipe</a>, this will ultimately allow us to stream commands from one terminal to another. Let's create a pipe named <code>/tmp/mypipe</code> which will create a file in this location:</p>
<pre><code class="language-bash">mkfifo /tmp/mypipe
</code></pre>
<h3 id="decide-your-set-of-commands"><a href="#decide-your-set-of-commands" aria-hidden="true"><span class="icon icon-link"></span></a>Decide your set of commands</h3>
<p>I started by choosing a few basic git commands, I saved the following to <code>$HOME/.config/voice2json/sentences.ini</code>:</p>
<pre><code>[~/status.sh]
status

[~/fetch.sh]
fetch

[~/diff.sh]
diff
</code></pre>
<p>which maps the speech "status", for example, to the intent <code>~/status.sh</code>, which is actually a file saved to my machine in this location which will be executed later.</p>
<p>The contents of the three files are:
<br><code>~/status.sh</code>:</p>
<pre><code class="language-bash">#!/bin/bash
git status
</code></pre>
<p><br><code>~/fetch.sh</code>:</p>
<pre><code class="language-bash">#!/bin/bash
git fetch --verbose
</code></pre>
<p><br><code>~/diff.sh</code>:</p>
<pre><code class="language-bash">#!/bin/bash
git diff --exit-code
</code></pre>
<p>Ensure these files are executable by running <code>sudo chmod +x &#x3C;file></code> on each of them.</p>
<h3 id="command-execution"><a href="#command-execution" aria-hidden="true"><span class="icon icon-link"></span></a>Command execution</h3>
<p>The following bash command will execute any lines that are written to the named pipe <code>/tmp/mypipe</code>, run this in a terminal within a git repository:</p>
<pre><code class="language-bash">tail -f /tmp/mypipe | sh &#x26;
</code></pre>
<h3 id="sending-intent-to-named-pipe"><a href="#sending-intent-to-named-pipe" aria-hidden="true"><span class="icon icon-link"></span></a>Sending intent to named pipe</h3>
<p>Open a second terminal and run:</p>
<pre><code class="language-bash">voice2json transcribe-stream | voice2json recognize-intent | (while read -r LINE; do echo "line is: $LINE"; echo "$LINE" | jq -r '.intent.name' > /tmp/mypipe; done;)
</code></pre>
<p>Effectively every time the voice2json tool recognizes an intent, the json blob is echoed to <code>stdout</code> but it is also piped to the <code>jq</code> tool which extracts the intent name, in this case the name of a file, and sends it to the named pipe <code>/tmp/mypipe</code>.</p>
<p>What you should then see if you say the word "status", for example, is for the command <code>git status</code> to be ran in the first terminal!</p>
<p><div style="width: 100%; margin: 0 0;"><div style="position: relative; padding-bottom: 56.25%; padding-top: 25px; height: 0;"><iframe style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/BPTqBYbZ0Bk"></iframe></div></div></p>
<h2 id="next-steps"><a href="#next-steps" aria-hidden="true"><span class="icon icon-link"></span></a>Next steps</h2>
<p>Now that we've proved the concept of being able to execute a one line bash command using only our voice, the concept easily extends to running python scripts, kicking off an <a href="https://www.ansible.com/" target="_blank" rel="nofollow noopener noreferrer">ansible</a> deploy or whatever takes your fancy.</p>
<p>This quick hack certainly isn't something you would want to rely on in a production environment. Voice2json does provide the ability to <a href="http://voice2json.org/commands.html#stream-events" target="_blank" rel="nofollow noopener noreferrer">publish intent recognition events</a> via <a href="https://mqtt.org/" target="_blank" rel="nofollow noopener noreferrer">MQTT</a> to a broker such as <a href="https://mosquitto.org/" target="_blank" rel="nofollow noopener noreferrer">Mosquitto</a>. A more robust solution would involve using an MQTT client to subscribe to these events in order to react to them. This can of course then extend your power into the realm of controlling IoT devices with your voice rather than just your terminal.</p>
<p>This setup could be used as a productivity enhancer for developers if used in the right way. This got me thinking that it would be cool to execute IDE shortcuts using your voice, my IDE of choice is <a href="https://code.visualstudio.com/" target="_blank" rel="nofollow noopener noreferrer">VS Code</a>, I would have to write a VS Code plugin in order to for the IDE to be able to interact with the voice2json tool. Before, going down this rabbit hole any further I did a quick search of available VS Code extensions to see if a voice command plugin already exists, that's when I found...</p>
<h2 id="seranadeai"><a href="#seranadeai" aria-hidden="true"><span class="icon icon-link"></span></a><a href="https://serenade.ai/" target="_blank" rel="nofollow noopener noreferrer">Seranade.ai</a></h2>
<p>Seranade claims to be:</p>
<blockquote>
<p>the most powerful way to program using natural speech. Boost your productivity by adding voice to your workflow.</p>
</blockquote>
<p>It takes the concept of software voice control and runs a mile with it, there are plugins for most popular IDEs and programming language support for Python, JavaScript, Java, C++, HTML and more. The execution of this is nothing short of professional, the <a href="https://serenade.ai/docs/" target="_blank" rel="nofollow noopener noreferrer">documentation</a> is a good indicator for the quality of this product. Not only can you use it to write code with your voice within your IDE but you can change window focus and browse the web with voice commands using the <a href="https://serenade.ai/docs/chrome/" target="_blank" rel="nofollow noopener noreferrer">Chrome extension</a>.</p>
<p>The free tier gives you pretty much everything other than data privacy. The on-boarding tutorial is a really pleasant experience, and every tiny detail has been thought about. You can even write you own <a href="https://serenade.ai/docs/api/" target="_blank" rel="nofollow noopener noreferrer">custom voice commands</a> or write your own <a href="https://serenade.ai/docs/protocol/" target="_blank" rel="nofollow noopener noreferrer">custom plugin</a> that integrates with Seranade.</p>
<p>It remains to be seen if I will integrate this into my day to day development workflow, at most I can see myself using it for a few shortcuts here and there as opposed to a throwing away my keyboard.</p>
<p>If like me you now work from home, it's a little bit more acceptable to talk to your computer whilst you work, however my dog keeps thinking I have a ball to throw when shouting "git fetch" at my computer...</p>
</div><div class="mb-8"><a href="/blog" class="font-bold uppercase">Back to Blog</a></div></div></div><footer class="bg-green-700 text-white"><div class="container mx-auto flex flex-col lg:flex-row items-center justify-between py-8"><div class="mb-8 lg:mb-0"></div><ul class="flex items-center"><li class="mr-8"><a href="mailto:mitch_10_4@hotmail.com" class="text-white hover:text-gray-400"><svg width="25" height="20" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M2.5 0h20A2.5 2.5 0 0 1 25 2.5v15a2.5 2.5 0 0 1-2.5 2.5h-20A2.5 2.5 0 0 1 0 17.5v-15C0 1.125 1.125 0 2.5 0zm20 4.225V2.5h-20v1.725l10 5 10-5zm0 2.8l-9.438 4.713a1.25 1.25 0 0 1-1.124 0L2.5 7.025V17.5h20V7.025z" fill-rule="nonzero"></path></svg></a></li><li class="mr-8"><a href="https://github.com/mitch104" target="_blank" class="text-white hover:text-gray-400"><svg width="24" height="24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a></li><li><a href="https://www.linkedin.com/in/mitchellharle/?originalSubdomain=uk" target="_blank" class="text-white hover:text-gray-400"><svg width="24" height="24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path></svg></a></li></ul></div></footer><div style="display:none"><svg id="dots-triangle" width="170" height="170" xmlns="http://www.w3.org/2000/svg"><path d="M168.152 170a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm-18.478-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm-18.478 0a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.479a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm-18.479 0a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0 18.479a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zM94.24 133.043a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0 18.479a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm-18.478 36.956a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0 18.479a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm-18.478 55.434a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0 18.479a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.479a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm-18.479 73.913a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0 18.479a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.479a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm-18.478 92.391a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0 18.479a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.479a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zM1.848 133.044a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.695zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0 18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-55.435a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.479a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.695 1.848 1.848 0 0 1 0 3.695zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696zm0-18.478a1.848 1.848 0 1 1 0-3.696 1.848 1.848 0 0 1 0 3.696z" fill="#2C8056" fill-rule="evenodd" opacity=".503"></path></svg></div></div>
    <script>window.__INITIAL_STATE__={"data":{"post":{"title":"How to execute terminal commands with your voice on Linux","date":"May 28, 2020","content":"\u003Cp\u003EThe open source NLP tool \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Evoice2json\u003C\u002Fa\u003E recently caught my eye on \u003Ca href=\"https:\u002F\u002Fnews.ycombinator.com\u002Fitem?id=27235970\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EHacker News\u003C\u002Fa\u003E:\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003Evoice2json is a collection of command-line tools for offline speech\u002Fintent recognition on Linux. It is free, open source (MIT), and supports 17 human languages.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003EIt got me thinking, how hard would it be use this tool to execute terminal commands with just your voice, it turns out... not very!\u003C\u002Fp\u003E\n\u003Ch2 id=\"installation\"\u003E\u003Ca href=\"#installation\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EInstallation\u003C\u002Fh2\u003E\n\u003Cp\u003ESee \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002Finstall.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Edocs\u003C\u002Fa\u003E for full details of voice2json installation instructions.\u003C\u002Fp\u003E\n\u003Cp\u003EOn my first attempt I used the \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002Finstall.html#docker-image\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EDocker image\u003C\u002Fa\u003E however I did encounter \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsynesthesiam\u002Fvoice2json\u002Fissues\u002F21\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Ethis issue\u003C\u002Fa\u003E with audio input being picked up by Docker, so for a smoother on-boarding process I recommend using the \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002Finstall.html#debian-package\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EDebian package\u003C\u002Fa\u003E which you can find in their \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsynesthesiam\u002Fvoice2json\u002Freleases\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EGitHub releases\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EYou will also need to \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002Finstall.html#download-profile\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Edownload a profile\u003C\u002Fa\u003E, for English I went for their recommendation of \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsynesthesiam\u002Fen-us_pocketsphinx-cmu\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EPocketsphinx\u003C\u002Fa\u003E which has served me well so far. Download the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsynesthesiam\u002Fen-us_pocketsphinx-cmu\u002Freleases\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003E.tar.gz file\u003C\u002Fa\u003E and extract it to \u003Ccode\u003E$HOME\u002F.config\u002Fvoice2json\u003C\u002Fcode\u003E. This is essentially your trained offline machine learning model.\u003C\u002Fp\u003E\n\u003Ch2 id=\"basic-usage\"\u003E\u003Ca href=\"#basic-usage\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EBasic usage\u003C\u002Fh2\u003E\n\u003Cp\u003EFirst, you will need to run the command:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Evoice2json train-profile\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003Ewhich should only take a second or two.\u003C\u002Fp\u003E\n\u003Cp\u003ETo check that everything's working run:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Evoice2json transcribe-stream --open\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003Eand this should transcribe your speech and output various related information in json format to \u003Ccode\u003Estdout\u003C\u002Fcode\u003E. Now, I have a Northern English accent so the transcriptions for me certainly weren't perfect, however the power comes from this tool when combining speech recognition with intent recognition.\u003C\u002Fp\u003E\n\u003Cp\u003EYou can define a set of phrases which relate to a given 'intent', so \"Turn on the living room lamp\" might be mapped to the intent \u003Ccode\u003EChangeLightState\u003C\u002Fcode\u003E. These intent mappings are defined in the file \u003Ccode\u003E$HOME\u002F.config\u002Fvoice2json\u002Fsentences.ini\u003C\u002Fcode\u003E and you can see an example \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsynesthesiam\u002Fen-us_pocketsphinx-cmu\u002Fblob\u002Fmaster\u002Fsentences.ini\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Ehere\u003C\u002Fa\u003E. For more details see \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002F#how-it-works\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Ehow it works\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003ESo the following command:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Evoice2json transcribe-stream | voice2json recognize-intent\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003Ewill try to match your spoken words to the closest matching phrase and hence intent. I found that when using a hand full of phrases the tool was very good at finding the right intent, even with my accent!\u003C\u002Fp\u003E\n\u003Ch2 id=\"how-to-execute-commands\"\u003E\u003Ca href=\"#how-to-execute-commands\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EHow to execute commands\u003C\u002Fh2\u003E\n\u003Cp\u003ENow that we've got the tool recognizing our intent from a set of possibilities how can we now use this to execute terminal commands?\u003C\u002Fp\u003E\n\u003Ch3 id=\"fifo-named-pipe\"\u003E\u003Ca href=\"#fifo-named-pipe\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EFifo named pipe\u003C\u002Fh3\u003E\n\u003Cp\u003EFirst off, we need to make a fifo (first in first out) \u003Ca href=\"https:\u002F\u002Fman7.org\u002Flinux\u002Fman-pages\u002Fman7\u002Ffifo.7.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Enamed pipe\u003C\u002Fa\u003E, this will ultimately allow us to stream commands from one terminal to another. Let's create a pipe named \u003Ccode\u003E\u002Ftmp\u002Fmypipe\u003C\u002Fcode\u003E which will create a file in this location:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Emkfifo \u002Ftmp\u002Fmypipe\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 id=\"decide-your-set-of-commands\"\u003E\u003Ca href=\"#decide-your-set-of-commands\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EDecide your set of commands\u003C\u002Fh3\u003E\n\u003Cp\u003EI started by choosing a few basic git commands, I saved the following to \u003Ccode\u003E$HOME\u002F.config\u002Fvoice2json\u002Fsentences.ini\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E[~\u002Fstatus.sh]\nstatus\n\n[~\u002Ffetch.sh]\nfetch\n\n[~\u002Fdiff.sh]\ndiff\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003Ewhich maps the speech \"status\", for example, to the intent \u003Ccode\u003E~\u002Fstatus.sh\u003C\u002Fcode\u003E, which is actually a file saved to my machine in this location which will be executed later.\u003C\u002Fp\u003E\n\u003Cp\u003EThe contents of the three files are:\n\u003Cbr\u003E\u003Ccode\u003E~\u002Fstatus.sh\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003E#!\u002Fbin\u002Fbash\ngit status\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cbr\u003E\u003Ccode\u003E~\u002Ffetch.sh\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003E#!\u002Fbin\u002Fbash\ngit fetch --verbose\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cbr\u003E\u003Ccode\u003E~\u002Fdiff.sh\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003E#!\u002Fbin\u002Fbash\ngit diff --exit-code\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EEnsure these files are executable by running \u003Ccode\u003Esudo chmod +x &#x3C;file\u003E\u003C\u002Fcode\u003E on each of them.\u003C\u002Fp\u003E\n\u003Ch3 id=\"command-execution\"\u003E\u003Ca href=\"#command-execution\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ECommand execution\u003C\u002Fh3\u003E\n\u003Cp\u003EThe following bash command will execute any lines that are written to the named pipe \u003Ccode\u003E\u002Ftmp\u002Fmypipe\u003C\u002Fcode\u003E, run this in a terminal within a git repository:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Etail -f \u002Ftmp\u002Fmypipe | sh &#x26;\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 id=\"sending-intent-to-named-pipe\"\u003E\u003Ca href=\"#sending-intent-to-named-pipe\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ESending intent to named pipe\u003C\u002Fh3\u003E\n\u003Cp\u003EOpen a second terminal and run:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Evoice2json transcribe-stream | voice2json recognize-intent | (while read -r LINE; do echo \"line is: $LINE\"; echo \"$LINE\" | jq -r '.intent.name' \u003E \u002Ftmp\u002Fmypipe; done;)\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EEffectively every time the voice2json tool recognizes an intent, the json blob is echoed to \u003Ccode\u003Estdout\u003C\u002Fcode\u003E but it is also piped to the \u003Ccode\u003Ejq\u003C\u002Fcode\u003E tool which extracts the intent name, in this case the name of a file, and sends it to the named pipe \u003Ccode\u003E\u002Ftmp\u002Fmypipe\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EWhat you should then see if you say the word \"status\", for example, is for the command \u003Ccode\u003Egit status\u003C\u002Fcode\u003E to be ran in the first terminal!\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cdiv style=\"width: 100%; margin: 0 0;\"\u003E\u003Cdiv style=\"position: relative; padding-bottom: 56.25%; padding-top: 25px; height: 0;\"\u003E\u003Ciframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" src=\"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002FBPTqBYbZ0Bk\"\u003E\u003C\u002Fiframe\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"next-steps\"\u003E\u003Ca href=\"#next-steps\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ENext steps\u003C\u002Fh2\u003E\n\u003Cp\u003ENow that we've proved the concept of being able to execute a one line bash command using only our voice, the concept easily extends to running python scripts, kicking off an \u003Ca href=\"https:\u002F\u002Fwww.ansible.com\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Eansible\u003C\u002Fa\u003E deploy or whatever takes your fancy.\u003C\u002Fp\u003E\n\u003Cp\u003EThis quick hack certainly isn't something you would want to rely on in a production environment. Voice2json does provide the ability to \u003Ca href=\"http:\u002F\u002Fvoice2json.org\u002Fcommands.html#stream-events\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Epublish intent recognition events\u003C\u002Fa\u003E via \u003Ca href=\"https:\u002F\u002Fmqtt.org\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EMQTT\u003C\u002Fa\u003E to a broker such as \u003Ca href=\"https:\u002F\u002Fmosquitto.org\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EMosquitto\u003C\u002Fa\u003E. A more robust solution would involve using an MQTT client to subscribe to these events in order to react to them. This can of course then extend your power into the realm of controlling IoT devices with your voice rather than just your terminal.\u003C\u002Fp\u003E\n\u003Cp\u003EThis setup could be used as a productivity enhancer for developers if used in the right way. This got me thinking that it would be cool to execute IDE shortcuts using your voice, my IDE of choice is \u003Ca href=\"https:\u002F\u002Fcode.visualstudio.com\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EVS Code\u003C\u002Fa\u003E, I would have to write a VS Code plugin in order to for the IDE to be able to interact with the voice2json tool. Before, going down this rabbit hole any further I did a quick search of available VS Code extensions to see if a voice command plugin already exists, that's when I found...\u003C\u002Fp\u003E\n\u003Ch2 id=\"seranadeai\"\u003E\u003Ca href=\"#seranadeai\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Fserenade.ai\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003ESeranade.ai\u003C\u002Fa\u003E\u003C\u002Fh2\u003E\n\u003Cp\u003ESeranade claims to be:\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003Ethe most powerful way to program using natural speech. Boost your productivity by adding voice to your workflow.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003EIt takes the concept of software voice control and runs a mile with it, there are plugins for most popular IDEs and programming language support for Python, JavaScript, Java, C++, HTML and more. The execution of this is nothing short of professional, the \u003Ca href=\"https:\u002F\u002Fserenade.ai\u002Fdocs\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Edocumentation\u003C\u002Fa\u003E is a good indicator for the quality of this product. Not only can you use it to write code with your voice within your IDE but you can change window focus and browse the web with voice commands using the \u003Ca href=\"https:\u002F\u002Fserenade.ai\u002Fdocs\u002Fchrome\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EChrome extension\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EThe free tier gives you pretty much everything other than data privacy. The on-boarding tutorial is a really pleasant experience, and every tiny detail has been thought about. You can even write you own \u003Ca href=\"https:\u002F\u002Fserenade.ai\u002Fdocs\u002Fapi\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Ecustom voice commands\u003C\u002Fa\u003E or write your own \u003Ca href=\"https:\u002F\u002Fserenade.ai\u002Fdocs\u002Fprotocol\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Ecustom plugin\u003C\u002Fa\u003E that integrates with Seranade.\u003C\u002Fp\u003E\n\u003Cp\u003EIt remains to be seen if I will integrate this into my day to day development workflow, at most I can see myself using it for a few shortcuts here and there as opposed to a throwing away my keyboard.\u003C\u002Fp\u003E\n\u003Cp\u003EIf like me you now work from home, it's a little bit more acceptable to talk to your computer whilst you work, however my dog keeps thinking I have a ball to throw when shouting \"git fetch\" at my computer...\u003C\u002Fp\u003E\n","tags":[{"title":"Bash","path":"\u002Ftag\u002FBash\u002F"},{"title":"Linux","path":"\u002Ftag\u002FLinux\u002F"},{"title":"Ubuntu","path":"\u002Ftag\u002FUbuntu\u002F"},{"title":"AI","path":"\u002Ftag\u002FAI\u002F"},{"title":"NLP","path":"\u002Ftag\u002FNLP\u002F"},{"title":"Open Source","path":"\u002Ftag\u002FOpen%20Source\u002F"}]}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.e423efc5.js" defer></script><script src="/assets/js/page--src--templates--post-vue.ab74464a.js" defer></script>
  </body>
</html>
