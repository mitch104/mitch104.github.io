{"hash":"2cbdc03282b13aa09057a95b3886c7e2d475e94d","data":{"post":{"title":"How to execute terminal commands with your voice on linux","date":"May 28, 2020","content":"<p>The open source NLP tool <a href=\"http://voice2json.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">voice2json</a> recently caught my eye on <a href=\"https://news.ycombinator.com/item?id=27235970\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Hacker News</a>:</p>\n<blockquote>\n<p>voice2json is a collection of command-line tools for offline speech/intent recognition on Linux. It is free, open source (MIT), and supports 17 human languages.</p>\n</blockquote>\n<p>It got me thinking, how hard would it be use this tool to execute terminal commands with just your voice, it turns out... not very!</p>\n<h2 id=\"installation\"><a href=\"#installation\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Installation</h2>\n<p>See <a href=\"http://voice2json.org/install.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">docs</a> for full details of voice2json installation instructions.</p>\n<p>On my first attempt I used the <a href=\"http://voice2json.org/install.html#docker-image\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Docker image</a> however I did encounter <a href=\"https://github.com/synesthesiam/voice2json/issues/21\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">this issue</a> with audio input being picked up by Docker, so for a smoother on-boarding process I recommend using the <a href=\"http://voice2json.org/install.html#debian-package\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Debian package</a> which you can find in their <a href=\"https://github.com/synesthesiam/voice2json/releases\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">GitHub releases</a>.</p>\n<p>You will also need to <a href=\"http://voice2json.org/install.html#download-profile\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">download a profile</a>, for English I went for their recommendation of <a href=\"https://github.com/synesthesiam/en-us_pocketsphinx-cmu\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Pocketsphinx</a> which has served me well so far. Download the <a href=\"https://github.com/synesthesiam/en-us_pocketsphinx-cmu/releases\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">.tar.gz file</a> and extract it to <code>$HOME/.config/voice2json</code>. This is essentially your trained offline machine learning model.</p>\n<h2 id=\"basic-usage\"><a href=\"#basic-usage\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Basic usage</h2>\n<p>First, you will need to run the command:</p>\n<pre><code class=\"language-bash\">voice2json train-profile\n</code></pre>\n<p>which should only take a second or two.</p>\n<p>To check that everything's working run:</p>\n<pre><code class=\"language-bash\">voice2json transcribe-stream --open\n</code></pre>\n<p>and this should transcribe your speech and output various related information in json format to <code>stdout</code>. Now, I have a Northern English accent so the transcriptions for me certainly weren't perfect, however the power comes from this tool when combining speech recognition with intent recognition.</p>\n<p>You can define a set of phrases which relate to a given 'intent', so \"Turn on the living room lamp\" might be mapped to the intent <code>ChangeLightState</code>. These intent mappings are defined in the file <code>$HOME/.config/voice2json/sentences.ini</code> and you can see an example <a href=\"https://github.com/synesthesiam/en-us_pocketsphinx-cmu/blob/master/sentences.ini\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">here</a>. For more details see <a href=\"http://voice2json.org/#how-it-works\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">how it works</a>.</p>\n<p>So the following command:</p>\n<pre><code class=\"language-bash\">voice2json transcribe-stream | voice2json recognize-intent\n</code></pre>\n<p>will try to match your spoken words to the closest matching phrase and hence intent. I found that when using a hand full of phrases the tool was very good at finding the right intent, even with my accent!</p>\n<h2 id=\"how-to-execute-commands\"><a href=\"#how-to-execute-commands\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>How to execute commands</h2>\n<p>Now that we've got the tool recognizing our intent from a set of possibilities how can we now use this to execute terminal commands?</p>\n<h3 id=\"fifo-named-pipe\"><a href=\"#fifo-named-pipe\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Fifo named pipe</h3>\n<p>First off, we need to make a fifo (first in first out) <a href=\"https://man7.org/linux/man-pages/man7/fifo.7.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">named pipe</a>, this will ultimately allow us to stream commands from one terminal to another. Let's create a pipe named <code>/tmp/mypipe</code> which will create a file in this location:</p>\n<pre><code class=\"language-bash\">mkfifo /tmp/mypipe\n</code></pre>\n<h3 id=\"decide-your-set-of-commands\"><a href=\"#decide-your-set-of-commands\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Decide your set of commands</h3>\n<p>I started by choosing a few basic git commands, I saved the following to <code>$HOME/.config/voice2json/sentences.ini</code>:</p>\n<pre><code>[~/status.sh]\nstatus\n\n[~/fetch.sh]\nfetch\n\n[~/diff.sh]\ndiff\n</code></pre>\n<p>which maps the speech \"status\", for example, to the intent <code>~/status.sh</code>, which is actually a file saved to my machine in this location which will be executed later.</p>\n<p>The contents of the three files are:\n<br><code>~/status.sh</code>:</p>\n<pre><code class=\"language-bash\">#!/bin/bash\ngit status\n</code></pre>\n<p><br><code>~/fetch.sh</code>:</p>\n<pre><code class=\"language-bash\">#!/bin/bash\ngit fetch --verbose\n</code></pre>\n<p><br><code>~/diff.sh</code>:</p>\n<pre><code class=\"language-bash\">#!/bin/bash\ngit diff --exit-code\n</code></pre>\n<p>Ensure these files are executable by running <code>sudo chmod +x &#x3C;file></code> on each of them.</p>\n<h3 id=\"command-execution\"><a href=\"#command-execution\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Command execution</h3>\n<p>The following bash command will execute any lines that are written to the named pipe <code>/tmp/mypipe</code>, run this in a terminal within a git repository:</p>\n<pre><code class=\"language-bash\">tail -f /tmp/mypipe | sh &#x26;\n</code></pre>\n<h3 id=\"sending-intent-to-named-pipe\"><a href=\"#sending-intent-to-named-pipe\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Sending intent to named pipe</h3>\n<p>Open a second terminal and run:</p>\n<pre><code class=\"language-bash\">voice2json transcribe-stream | voice2json recognize-intent | (while read -r LINE; do echo \"line is: $LINE\"; echo \"$LINE\" | jq -r '.intent.name' > /tmp/mypipe; done;)\n</code></pre>\n<p>Effectively every time the voice2json tool recognizes an intent, the json blob is echoed to <code>stdout</code> but it is also piped to the <code>jq</code> tool which extracts the intent name, in this case the name of a file, and sends it to the named pipe <code>/tmp/mypipe</code>.</p>\n<p>What you should then see if you say the word \"status\", for example, is for the command <code>git status</code> to be ran in the first terminal!</p>\n<p><div style=\"width: 100%; margin: 0 0;\"><div style=\"position: relative; padding-bottom: 56.25%; padding-top: 25px; height: 0;\"><iframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" src=\"https://www.youtube.com/embed/BPTqBYbZ0Bk\"></iframe></div></div></p>\n<h2 id=\"next-steps\"><a href=\"#next-steps\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Next steps</h2>\n<p>Now that we've proved the concept of being able to execute a one line bash command using only our voice, the concept easily extends to running python scripts, kicking off an <a href=\"https://www.ansible.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ansible</a> deploy or whatever takes your fancy.</p>\n<p>This quick hack certainly isn't something you would want to rely on in a production environment. Voice2json does provide the ability to <a href=\"http://voice2json.org/commands.html#stream-events\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">publish intent recognition events</a> via <a href=\"https://mqtt.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">MQTT</a> to a broker such as <a href=\"https://mosquitto.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Mosquitto</a>. A more robust solution would involve using an MQTT client to subscribe to these events in order to react to them. This can of course then extend your power into the realm of controlling IoT devices with your voice rather than just your terminal.</p>\n<p>This setup could be used as a productivity enhancer for developers if used in the right way. This got me thinking that it would be cool to execute IDE shortcuts using your voice, my IDE of choice is <a href=\"https://code.visualstudio.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">VS Code</a>, I would have to write a VS Code plugin in order to for the IDE to be able to interact with the voice2json tool. Before, going down this rabbit hole any further I did a quick search of available VS Code extensions to see if a voice command plugin already exists, that's when I found...</p>\n<h2 id=\"seranadeai\"><a href=\"#seranadeai\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a><a href=\"https://serenade.ai/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Seranade.ai</a></h2>\n<p>Seranade claims to be:</p>\n<blockquote>\n<p>the most powerful way to program using natural speech. Boost your productivity by adding voice to your workflow.</p>\n</blockquote>\n<p>It takes the concept of software voice control and runs a mile with it, there are plugins for most popular IDEs and programming language support for Python, JavaScript, Java, C++, HTML and more. The execution of this is nothing short of professional, the <a href=\"https://serenade.ai/docs/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">documentation</a> is a good indicator for the quality of this product. Not only can you use it to write code with your voice within your IDE but you can change window focus and browse the web with voice commands using the <a href=\"https://serenade.ai/docs/chrome/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Chrome extension</a>.</p>\n<p>The free tier gives you pretty much everything other than data privacy. The on-boarding tutorial is a really pleasant experience, and every tiny detail has been thought about. You can even write you own <a href=\"https://serenade.ai/docs/api/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">custom voice commands</a> or write your own <a href=\"https://serenade.ai/docs/protocol/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">custom plugin</a> that integrates with Seranade.</p>\n<p>It remains to be seen if I will integrate this into my day to day development workflow, at most I can see myself using it for a few shortcuts here and there as opposed to a throwing away my keyboard.</p>\n<p>If like me you now work from home, it's a little bit more acceptable to talk to your computer whilst you work, however my dog keeps thinking I have a ball to throw when shouting \"git fetch\" at my computer...</p>\n","tags":[{"title":"Bash","path":"/tag/Bash/"},{"title":"Linux","path":"/tag/Linux/"},{"title":"Ubuntu","path":"/tag/Ubuntu/"},{"title":"AI","path":"/tag/AI/"},{"title":"NLP","path":"/tag/NLP/"},{"title":"Open Source","path":"/tag/Open%20Source/"}]}},"context":{}}